{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7b45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageFile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6466ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "DATASETS_DIR = Path(\"../datasets\")\n",
    "\n",
    "SRC_ROOT = DATASETS_DIR / \"extended_dataset\"  # merged source (train + test)\n",
    "SRC_TRAIN = SRC_ROOT / \"train\"\n",
    "SRC_TEST  = SRC_ROOT / \"test\"\n",
    "\n",
    "OUT_SIZES = [224, 232, 384]  # datasets/dataset_224, dataset_384\n",
    "USE_SHORT_SIDE_RESIZE = [False, True, True]\n",
    "TEST_SUBFOLDER = \"unknown\"   # for ImageFolder\n",
    "\n",
    "# JPEG settings\n",
    "JPEG_QUALITY = 92\n",
    "JPEG_OPTIMIZE = True\n",
    "JPEG_PROGRESSIVE = True\n",
    "\n",
    "# Performance\n",
    "NUM_WORKERS = os.cpu_count() or 8  # adjust if needed\n",
    "\n",
    "# If True, skip if destination file already exists\n",
    "SKIP_EXISTING = True\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "VALID_EXTS = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e754bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(p: Path) -> bool:\n",
    "    return p.is_file() and p.suffix.lower() in VALID_EXTS\n",
    "\n",
    "def safe_stem(name: str) -> str:\n",
    "    # Keep filename stable but avoid weird characters\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._- \" else \"_\" for ch in name).strip()\n",
    "\n",
    "def unique_name_from_path(src: Path) -> str:\n",
    "    \"\"\"\n",
    "    When converting everything to .jpg, collisions can happen (same stem from different files).\n",
    "    We make it stable by appending a short hash based on full relative path.\n",
    "    \"\"\"\n",
    "    rel = str(src)\n",
    "    h = hashlib.sha1(rel.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    return f\"{safe_stem(src.stem)}_{h}.jpg\"\n",
    "\n",
    "def load_rgb_image(src: Path) -> Image.Image:\n",
    "    # Convert to RGB reliably (handles grayscale, palette, RGBA, etc.)\n",
    "    with Image.open(src) as im:\n",
    "        return im.convert(\"RGB\")\n",
    "\n",
    "def resize_to_square(im: Image.Image, size: int) -> Image.Image:\n",
    "    # Direct resize to (size, size). (No aspect preservation; matches your training Resize.)\n",
    "    return im.resize((size, size), resample=Image.BILINEAR)\n",
    "\n",
    "def resize_shorter_side(im: Image.Image, short_side: int) -> Image.Image:\n",
    "    w, h = im.size\n",
    "    if w == 0 or h == 0:\n",
    "        raise ValueError(f\"Invalid image size: {im.size}\")\n",
    "    if w < h:\n",
    "        new_w = short_side\n",
    "        new_h = int(round(h * (short_side / w)))\n",
    "    else:\n",
    "        new_h = short_side\n",
    "        new_w = int(round(w * (short_side / h)))\n",
    "\n",
    "    return im.resize((new_w, new_h), resample=Image.BILINEAR)\n",
    "\n",
    "def process_one(src: Path, dst: Path, short_side_resize: bool, size: int) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Returns (ok, message). Writes JPEG to dst.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if SKIP_EXISTING and dst.exists():\n",
    "            return True, \"skipped\"\n",
    "\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        im = load_rgb_image(src)\n",
    "        if short_side_resize:\n",
    "            im = resize_shorter_side(im, size)\n",
    "        else:\n",
    "            im = resize_to_square(im, size)\n",
    "\n",
    "        im.save(\n",
    "            dst,\n",
    "            format=\"JPEG\",\n",
    "            quality=JPEG_QUALITY,\n",
    "            optimize=JPEG_OPTIMIZE,\n",
    "            progressive=JPEG_PROGRESSIVE,\n",
    "        )\n",
    "        return True, \"ok\"\n",
    "    except Exception as e:\n",
    "        return False, f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "def build_file_list_train(src_train: Path):\n",
    "    \"\"\"\n",
    "    Returns list of (src_path, class_name)\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for cls_dir in sorted([p for p in src_train.iterdir() if p.is_dir()]):\n",
    "        cls = cls_dir.name\n",
    "        for p in cls_dir.rglob(\"*\"):\n",
    "            if is_image_file(p):\n",
    "                items.append((p, cls))\n",
    "    return items\n",
    "\n",
    "def build_file_list_test(src_test: Path):\n",
    "    \"\"\"\n",
    "    Returns list of src image paths. Works if test is either flat or already under unknown/.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for p in src_test.rglob(\"*\"):\n",
    "        if is_image_file(p):\n",
    "            items.append(p)\n",
    "    return items\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main routine\n",
    "# -------------------------\n",
    "def build_resized_datasets():\n",
    "    # Gather source files once\n",
    "    train_items = build_file_list_train(SRC_TRAIN)\n",
    "    test_items  = build_file_list_test(SRC_TEST)\n",
    "\n",
    "    print(f\"Found train images: {len(train_items)}\")\n",
    "    print(f\"Found test  images: {len(test_items)}\")\n",
    "\n",
    "    for short_side_resize, size in zip(USE_SHORT_SIDE_RESIZE, OUT_SIZES):\n",
    "        out_root = DATASETS_DIR / f\"dataset_{size}\"\n",
    "        out_train = out_root / \"train\"\n",
    "        out_test  = out_root / \"test\" / TEST_SUBFOLDER\n",
    "\n",
    "        # Prepare dirs\n",
    "        out_train.mkdir(parents=True, exist_ok=True)\n",
    "        out_test.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # -------- Train --------\n",
    "        futures = []\n",
    "        ok_cnt = 0\n",
    "        bad = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as ex:\n",
    "            for src, cls in train_items:\n",
    "                # preserve class folder\n",
    "                dst = out_train / cls / unique_name_from_path(src)\n",
    "                futures.append(ex.submit(process_one, src, dst, short_side_resize, size))\n",
    "\n",
    "            for f in tqdm(as_completed(futures), total=len(futures), desc=f\"train_{size}\"):\n",
    "                ok, msg = f.result()\n",
    "                if ok:\n",
    "                    ok_cnt += 1\n",
    "                else:\n",
    "                    bad.append(msg)\n",
    "\n",
    "        print(f\"[dataset_{size}] train done: ok={ok_cnt}/{len(futures)}  failed={len(bad)}\")\n",
    "\n",
    "        # -------- Test --------\n",
    "        futures = []\n",
    "        ok_cnt = 0\n",
    "        bad = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as ex:\n",
    "            for src in test_items:\n",
    "                dst = out_test / unique_name_from_path(src)\n",
    "                futures.append(ex.submit(process_one, src, dst, short_side_resize, size))\n",
    "\n",
    "            for f in tqdm(as_completed(futures), total=len(futures), desc=f\"test_{size}\"):\n",
    "                ok, msg = f.result()\n",
    "                if ok:\n",
    "                    ok_cnt += 1\n",
    "                else:\n",
    "                    bad.append(msg)\n",
    "\n",
    "        print(f\"[dataset_{size}] test  done: ok={ok_cnt}/{len(futures)}  failed={len(bad)}\")\n",
    "\n",
    "        print(f\"✅ Finished dataset_{size} at: {out_root.resolve()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c18e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found train images: 14687\n",
      "Found test  images: 849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_224:   1%|          | 151/14687 [00:01<02:00, 120.50it/s]/home/lighter_01/.pyenv/versions/wsl_main/lib/python3.11/site-packages/PIL/Image.py:1054: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "train_224:  47%|████▋     | 6925/14687 [01:06<01:51, 69.31it/s] /home/lighter_01/.pyenv/versions/wsl_main/lib/python3.11/site-packages/PIL/Image.py:3406: DecompressionBombWarning: Image size (97653348 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "train_224: 100%|██████████| 14687/14687 [02:35<00:00, 94.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset_224] train done: ok=14687/14687  failed=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_224: 100%|██████████| 849/849 [00:03<00:00, 236.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset_224] test  done: ok=849/849  failed=0\n",
      "✅ Finished dataset_224 at: /home/lighter_01/projects/itmo/computer_vision/lab3_classif/datasets/dataset_224\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_232: 100%|██████████| 14687/14687 [02:37<00:00, 93.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset_232] train done: ok=14687/14687  failed=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_232: 100%|██████████| 849/849 [00:03<00:00, 237.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset_232] test  done: ok=849/849  failed=0\n",
      "✅ Finished dataset_232 at: /home/lighter_01/projects/itmo/computer_vision/lab3_classif/datasets/dataset_232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_384: 100%|██████████| 14687/14687 [02:39<00:00, 91.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset_384] train done: ok=14687/14687  failed=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_384: 100%|██████████| 849/849 [00:03<00:00, 222.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dataset_384] test  done: ok=849/849  failed=0\n",
      "✅ Finished dataset_384 at: /home/lighter_01/projects/itmo/computer_vision/lab3_classif/datasets/dataset_384\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "build_resized_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e08910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl python 3.11.5",
   "language": "python",
   "name": "wsl_main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
