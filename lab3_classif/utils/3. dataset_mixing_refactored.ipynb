{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset mixing / merging notebook\n",
        "This notebook **merges** your original dataset (`datasets/splitted/`) with newly scraped images\n",
        "into `datasets/extended_dataset/`.\n",
        "\n",
        "It performs:\n",
        "1. Copy **train** images from `splitted/train/<class>/` into `extended_dataset/train/<class>/`.\n",
        "2. Keep any **scraped** images already present in `extended_dataset/train/<class>/`.\n",
        "   (After running, `extended_dataset/train` contains original + scraped.)\n",
        "3. Copy **test** images from `splitted/test/` into `extended_dataset/test/<single_subfolder>/`\n",
        "   so PyTorch `ImageFolder` can read them.\n",
        "\n",
        "Notes:\n",
        "- Copy-only (no resizing, no filtering).\n",
        "- Avoids filename collisions by appending a short hash suffix when needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from typing import Iterable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Paths & settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SRC_TRAIN: /home/lighter_01/projects/itmo/computer_vision/lab3_classif/datasets/splitted/train\n",
            "DST_TRAIN: /home/lighter_01/projects/itmo/computer_vision/lab3_classif/datasets/extended_dataset/train\n",
            "SRC_TEST : /home/lighter_01/projects/itmo/computer_vision/lab3_classif/datasets/splitted/test\n",
            "DST_TEST : /home/lighter_01/projects/itmo/computer_vision/lab3_classif/datasets/extended_dataset/test\n"
          ]
        }
      ],
      "source": [
        "ROOT = Path(\"..\") / \"datasets\"\n",
        "\n",
        "SPLITTED_DIR = ROOT / \"splitted\"\n",
        "EXTENDED_DIR = ROOT / \"extended_dataset\"\n",
        "\n",
        "SRC_TRAIN = SPLITTED_DIR / \"train\"\n",
        "SRC_TEST  = SPLITTED_DIR / \"test\"\n",
        "\n",
        "DST_TRAIN = EXTENDED_DIR / \"train\"\n",
        "DST_TEST  = EXTENDED_DIR / \"test\"\n",
        "\n",
        "# Put all test images into this subfolder so torchvision.datasets.ImageFolder works.\n",
        "# (ImageFolder expects at least one class-like subfolder.)\n",
        "TEST_SUBFOLDER_NAME = \"unknown\"\n",
        "\n",
        "# Preserve file metadata (mtime) via copy2\n",
        "USE_COPY2 = True\n",
        "\n",
        "# If True, skip copying when an identical-size file already exists at destination.\n",
        "SKIP_IF_EXISTS = True\n",
        "\n",
        "print(\"SRC_TRAIN:\", SRC_TRAIN.resolve())\n",
        "print(\"DST_TRAIN:\", DST_TRAIN.resolve())\n",
        "print(\"SRC_TEST :\", SRC_TEST.resolve())\n",
        "print(\"DST_TEST :\", DST_TEST.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def file_sha1_prefix(path: Path, nbytes: int = 1 << 20) -> str:\n",
        "    \"\"\"Hash the first nbytes (default 1MB) for a short collision suffix.\"\"\"\n",
        "    h = hashlib.sha1()\n",
        "    with open(path, \"rb\") as f:\n",
        "        h.update(f.read(nbytes))\n",
        "    return h.hexdigest()[:8]\n",
        "\n",
        "def ensure_dir(p: Path) -> None:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def copy_file_unique(src: Path, dst_dir: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Copy src into dst_dir. If a file with the same name exists, append a hash suffix.\n",
        "\n",
        "    Returns the final destination path.\n",
        "    \"\"\"\n",
        "    ensure_dir(dst_dir)\n",
        "    dst = dst_dir / src.name\n",
        "\n",
        "    # Fast path: no collision\n",
        "    if not dst.exists():\n",
        "        shutil.copy2(src, dst) if USE_COPY2 else shutil.copy(src, dst)\n",
        "        return dst\n",
        "\n",
        "    # If exists and skip is enabled: skip if size matches\n",
        "    if SKIP_IF_EXISTS:\n",
        "        try:\n",
        "            if dst.stat().st_size == src.stat().st_size:\n",
        "                return dst\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "    # Collision: add suffix before extension\n",
        "    suffix = file_sha1_prefix(src)\n",
        "    stem = src.stem\n",
        "    ext = src.suffix  # includes dot\n",
        "    candidate = dst_dir / f\"{stem}_{suffix}{ext}\"\n",
        "\n",
        "    i = 1\n",
        "    while candidate.exists():\n",
        "        candidate = dst_dir / f\"{stem}_{suffix}_{i}{ext}\"\n",
        "        i += 1\n",
        "\n",
        "    shutil.copy2(src, candidate) if USE_COPY2 else shutil.copy(src, candidate)\n",
        "    return candidate\n",
        "\n",
        "def iter_files(root: Path) -> Iterable[Path]:\n",
        "    for p in root.rglob(\"*\"):\n",
        "        if p.is_file():\n",
        "            yield p\n",
        "\n",
        "def class_folders(root: Path) -> list[Path]:\n",
        "    return sorted([p for p in root.iterdir() if p.is_dir()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Merge train set (splitted/train â†’ extended_dataset/train)\n",
        "Copies **all original** train images into the extended train directory. Any scraped images already present remain there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_train(src_train: Path, dst_train: Path) -> dict[str, tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    For each class folder in src_train, copy files into dst_train/<class>.\n",
        "    Returns dict: class -> (copied_count, skipped_or_existing_count)\n",
        "    \"\"\"\n",
        "    ensure_dir(dst_train)\n",
        "    stats: dict[str, tuple[int, int]] = {}\n",
        "\n",
        "    for cls_dir in class_folders(src_train):\n",
        "        cls_name = cls_dir.name\n",
        "        dst_cls = dst_train / cls_name\n",
        "        ensure_dir(dst_cls)\n",
        "\n",
        "        copied = 0\n",
        "        skipped = 0\n",
        "\n",
        "        for src_file in iter_files(cls_dir):\n",
        "            # Preserve relative structure inside class folder (if there are subfolders)\n",
        "            rel_parent = src_file.parent.relative_to(cls_dir)\n",
        "            dst_dir = dst_cls / rel_parent\n",
        "\n",
        "            before = (dst_dir / src_file.name)\n",
        "            existed_before = before.exists()\n",
        "\n",
        "            final_dst = copy_file_unique(src_file, dst_dir)\n",
        "\n",
        "            if existed_before and final_dst.name == src_file.name and SKIP_IF_EXISTS:\n",
        "                try:\n",
        "                    if final_dst.stat().st_size == src_file.stat().st_size:\n",
        "                        skipped += 1\n",
        "                        continue\n",
        "                except OSError:\n",
        "                    pass\n",
        "\n",
        "            copied += 1\n",
        "\n",
        "        stats[cls_name] = (copied, skipped)\n",
        "        print(f\"{cls_name}: copied={copied}, skipped/existing={skipped}\")\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ee3787d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ace: copied=168, skipped/existing=0\n",
            "Akainu: copied=167, skipped/existing=0\n",
            "Brook: copied=178, skipped/existing=0\n",
            "Chopper: copied=170, skipped/existing=0\n",
            "Crocodile: copied=167, skipped/existing=0\n",
            "Franky: copied=170, skipped/existing=0\n",
            "Jinbei: copied=166, skipped/existing=0\n",
            "Kurohige: copied=170, skipped/existing=0\n",
            "Law: copied=175, skipped/existing=0\n",
            "Luffy: copied=97, skipped/existing=0\n",
            "Mihawk: copied=167, skipped/existing=0\n",
            "Nami: copied=181, skipped/existing=0\n",
            "Rayleigh: copied=167, skipped/existing=0\n",
            "Robin: copied=167, skipped/existing=0\n",
            "Sanji: copied=135, skipped/existing=0\n",
            "Shanks: copied=168, skipped/existing=0\n",
            "Usopp: copied=170, skipped/existing=0\n",
            "Zoro: copied=132, skipped/existing=0\n"
          ]
        }
      ],
      "source": [
        "train_stats = merge_train(SRC_TRAIN, DST_TRAIN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Prepare test set for PyTorch ImageFolder\n",
        "`torchvision.datasets.ImageFolder` expects: `root/class_x/xxx.png`.\n",
        "Your test set is unlabeled, so we put everything under one folder: `test/unknown/`.\n",
        "\n",
        "This copies **all files** from `splitted/test/` (recursively) into that folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_test(src_test: Path, dst_test: Path, subfolder: str = TEST_SUBFOLDER_NAME) -> tuple[int, int]:\n",
        "    ensure_dir(dst_test)\n",
        "    dst_bucket = dst_test / subfolder\n",
        "    ensure_dir(dst_bucket)\n",
        "\n",
        "    copied = 0\n",
        "    skipped = 0\n",
        "\n",
        "    for src_file in iter_files(src_test):\n",
        "        before = dst_bucket / src_file.name\n",
        "        existed_before = before.exists()\n",
        "\n",
        "        final_dst = copy_file_unique(src_file, dst_bucket)\n",
        "\n",
        "        if existed_before and final_dst.name == src_file.name and SKIP_IF_EXISTS:\n",
        "            try:\n",
        "                if final_dst.stat().st_size == src_file.stat().st_size:\n",
        "                    skipped += 1\n",
        "                    continue\n",
        "            except OSError:\n",
        "                pass\n",
        "\n",
        "        copied += 1\n",
        "\n",
        "    print(f\"Test: copied={copied}, skipped/existing={skipped}, into: {dst_bucket}\")\n",
        "    return copied, skipped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d3bb29c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test: copied=849, skipped/existing=0, into: ../datasets/extended_dataset/test/unknown\n"
          ]
        }
      ],
      "source": [
        "test_stats = prepare_test(SRC_TEST, DST_TEST, TEST_SUBFOLDER_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Quick sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extended train files: 14687\n",
            "Extended test  files: 849\n",
            "Classes match: True\n"
          ]
        }
      ],
      "source": [
        "def count_files(root: Path) -> int:\n",
        "    return sum(1 for _ in iter_files(root))\n",
        "\n",
        "print(\"Extended train files:\", count_files(DST_TRAIN))\n",
        "print(\"Extended test  files:\", count_files(DST_TEST))\n",
        "\n",
        "src_classes = [p.name for p in class_folders(SRC_TRAIN)]\n",
        "dst_classes = [p.name for p in class_folders(DST_TRAIN)]\n",
        "print(\"Classes match:\", src_classes == dst_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cdb4933",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "wsl python 3.11.5",
      "language": "python",
      "name": "wsl_main"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
