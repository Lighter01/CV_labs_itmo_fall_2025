{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Danbooru character image scraper (dev notebook)\n",
        "This notebook downloads **.jpg** and **.png** images for each character tag into\n",
        "`datasets/extended_dataset/train/<class_name>/`.\n",
        "\n",
        "**Notes**\n",
        "- This notebook **only scrapes** images; it **does not merge** with your original dataset.\n",
        "- Danbooru read endpoints are globally rate-limited (10 req/s), and pagination/limits apply. See the API help page for details. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import shutil\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Iterable, Tuple\n",
        "from collections import Counter\n",
        "\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "7c69dcd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "Image.MAX_IMAGE_PIXELS = 933120000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Configuration\n",
        "Put your credentials in environment variables to avoid hardcoding them in the notebook:\n",
        "\n",
        "- `DANBOORU_USERNAME`\n",
        "- `DANBOORU_API_KEY`\n",
        "\n",
        "Danbooru supports authentication either via `login` / `api_key` query params or HTTP Basic Auth. Below we use HTTP Basic Auth (cleaner)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OUT_TRAIN_DIR: /home/lighter_01/projects/itmo/computer_vision/lab3_classif/datasets/extended_dataset/train\n"
          ]
        }
      ],
      "source": [
        "# --- Danbooru endpoints ---\n",
        "BASE_URL = \"https://danbooru.donmai.us\"   # main\n",
        "# BASE_URL = \"https://testbooru.donmai.us\"  # safer for testing\n",
        "\n",
        "# --- Credentials (recommended: env vars) ---\n",
        "USERNAME = os.environ.get(\"DANBOORU_USERNAME\", \"\").strip()\n",
        "API_KEY  = os.environ.get(\"DANBOORU_API_KEY\", \"\").strip()\n",
        "\n",
        "if not USERNAME or not API_KEY:\n",
        "    print(\"⚠️ Set DANBOORU_USERNAME and DANBOORU_API_KEY environment variables before scraping.\")\n",
        "\n",
        "AUTH = aiohttp.BasicAuth(USERNAME, API_KEY) if USERNAME and API_KEY else None\n",
        "\n",
        "# --- Dataset paths ---\n",
        "SPLITTED_TRAIN_DIR = Path(\"..\") / \"datasets\" / \"splitted\" / \"train\"\n",
        "OUT_TRAIN_DIR      = Path(\"..\") / \"datasets\" / \"extended_dataset\" / \"train\"\n",
        "OUT_TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Scrape policy ---\n",
        "ALLOWED_EXTS = {\"jpg\", \"jpeg\", \"png\"}  # download jpg/png only\n",
        "EXCLUDE_RATING = \"e\"                   # exclude explicit\n",
        "SINGLE_CHAR_ONLY = True                # chartags:1\n",
        "\n",
        "# --- Optional quality filters ---\n",
        "# Minimum image dimensions. Set to 0 to disable a dimension constraint.\n",
        "MIN_WIDTH = 512\n",
        "MIN_HEIGHT = 512\n",
        "\n",
        "# --- Request limits / throttling ---\n",
        "POSTS_LIMIT_PER_PAGE = 200        # max for /posts.json\n",
        "MAX_READ_RPS = 8                  # keep below Danbooru's 10 req/s read limit\n",
        "MAX_CONN = 16                     # TCP connections\n",
        "MAX_DL_CONCURRENCY = 12           # concurrent downloads (not requests per second)\n",
        "REQUEST_TIMEOUT_S = 20\n",
        "MAX_RETRIES = 4\n",
        "\n",
        "# --- Dedup / overwrite behavior ---\n",
        "SKIP_IF_EXISTS = True             # if file already exists in output dir, don't re-download\n",
        "\n",
        "print(\"OUT_TRAIN_DIR:\", OUT_TRAIN_DIR.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Classes and character tags\n",
        "We read your class folder names from `datasets/splitted/train` and create matching output folders.\n",
        "\n",
        "You said you'll **manually align** `character_tags_list` order with `characters_list` to keep the mapping simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['Ace', 'Akainu', 'Brook', 'Chopper', 'Crocodile', 'Franky', 'Jinbei', 'Kurohige', 'Law', 'Luffy', 'Mihawk', 'Nami', 'Rayleigh', 'Robin', 'Sanji', 'Shanks', 'Usopp', 'Zoro']\n",
            "Num classes: 18\n"
          ]
        }
      ],
      "source": [
        "# Read folder names (stable sort)\n",
        "characters_list = sorted([p.name for p in SPLITTED_TRAIN_DIR.iterdir() if p.is_dir()])\n",
        "print(\"Classes:\", characters_list)\n",
        "print(\"Num classes:\", len(characters_list))\n",
        "\n",
        "# Create output folders\n",
        "for cls in characters_list:\n",
        "    (OUT_TRAIN_DIR / cls).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Ace': 'portgas_d._ace',\n",
              " 'Akainu': 'sakazuki_(one_piece)',\n",
              " 'Brook': 'brook_(one_piece)',\n",
              " 'Chopper': 'tony_tony_chopper',\n",
              " 'Crocodile': 'crocodile_(one_piece)',\n",
              " 'Franky': 'franky_(one_piece)',\n",
              " 'Jinbei': 'jinbe_(one_piece)',\n",
              " 'Kurohige': 'marshall_d._teach',\n",
              " 'Law': 'trafalgar_law',\n",
              " 'Luffy': 'monkey_d._luffy',\n",
              " 'Mihawk': 'dracule_mihawk',\n",
              " 'Nami': 'nami_(one_piece)',\n",
              " 'Rayleigh': 'silvers_rayleigh',\n",
              " 'Robin': 'nico_robin',\n",
              " 'Sanji': 'sanji_(one_piece)',\n",
              " 'Shanks': 'shanks_(one_piece)',\n",
              " 'Usopp': 'usopp',\n",
              " 'Zoro': 'roronoa_zoro'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Put Danbooru character tags here IN THE SAME ORDER as characters_list.\n",
        "character_tags_list = [\n",
        "    'portgas_d._ace',\n",
        "    'sakazuki_(one_piece)',\n",
        "    'brook_(one_piece)',\n",
        "    'tony_tony_chopper',\n",
        "    'crocodile_(one_piece)',\n",
        "    'franky_(one_piece)',\n",
        "    'jinbe_(one_piece)',\n",
        "    'marshall_d._teach',\n",
        "    'trafalgar_law',\n",
        "    'monkey_d._luffy',\n",
        "    'dracule_mihawk',\n",
        "    'nami_(one_piece)',\n",
        "    'silvers_rayleigh',\n",
        "    'nico_robin',\n",
        "    'sanji_(one_piece)',\n",
        "    'shanks_(one_piece)',\n",
        "    'usopp',\n",
        "    'roronoa_zoro'\n",
        "]\n",
        "\n",
        "assert len(character_tags_list) == len(characters_list), (\n",
        "    f\"Tags list length {len(character_tags_list)} must match classes length {len(characters_list)}\"\n",
        ")\n",
        "\n",
        "characters_dict = dict(zip(characters_list, character_tags_list))\n",
        "characters_dict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Danbooru API helpers\n",
        "We use:\n",
        "- `GET /counts/posts.json` to get how many posts match tags.\n",
        "- `GET /posts.json` to enumerate posts (max 200 per page).\n",
        "\n",
        "We also implement retries, backoff, and handle throttling (HTTP 429)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5393ee7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class ScrapeQuery:\n",
        "    tag: str\n",
        "    exclude_rating: str = EXCLUDE_RATING\n",
        "    single_char_only: bool = SINGLE_CHAR_ONLY\n",
        "\n",
        "    def tag_string(self) -> str:\n",
        "        parts = [self.tag, f\"-rating:{self.exclude_rating}\"]\n",
        "        if self.single_char_only:\n",
        "            parts.append(\"chartags:1\")\n",
        "        return \" \".join(parts)\n",
        "\n",
        "def _sleep_backoff(attempt: int, base: float = 1.0, cap: float = 20.0) -> float:\n",
        "    # exponential backoff with cap\n",
        "    return min(cap, base * (2 ** attempt))\n",
        "\n",
        "class RateLimiter:\n",
        "    \"\"\"Simple token-ish limiter: ensures we don't exceed MAX_READ_RPS.\"\"\"\n",
        "    def __init__(self, rps: float):\n",
        "        self.min_interval = 1.0 / float(rps)\n",
        "        self._last = 0.0\n",
        "        self._lock = asyncio.Lock()\n",
        "\n",
        "    async def wait(self):\n",
        "        async with self._lock:\n",
        "            now = time.perf_counter()\n",
        "            wait = self.min_interval - (now - self._last)\n",
        "            if wait > 0:\n",
        "                await asyncio.sleep(wait)\n",
        "            self._last = time.perf_counter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "698d24e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "rate_limiter = RateLimiter(MAX_READ_RPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def danbooru_get_json(\n",
        "    session: aiohttp.ClientSession,\n",
        "    path: str,\n",
        "    params: dict,\n",
        "    *,\n",
        "    timeout_s: int = REQUEST_TIMEOUT_S,\n",
        "    max_retries: int = MAX_RETRIES,\n",
        ") -> Optional[dict | list]:\n",
        "    url = f\"{BASE_URL}{path}\"\n",
        "    for attempt in range(max_retries):\n",
        "        await rate_limiter.wait()\n",
        "        try:\n",
        "            async with session.get(url, params=params, timeout=timeout_s) as resp:\n",
        "                # Handle throttling / transient errors\n",
        "                if resp.status == 429:\n",
        "                    retry_after = resp.headers.get(\"Retry-After\")\n",
        "                    delay = float(retry_after) if retry_after else _sleep_backoff(attempt, base=2.0)\n",
        "                    print(f\"429 throttled. Sleeping {delay:.1f}s for {url}\")\n",
        "                    await asyncio.sleep(delay)\n",
        "                    continue\n",
        "                if resp.status in (502, 503, 500):\n",
        "                    delay = _sleep_backoff(attempt, base=2.0)\n",
        "                    print(f\"{resp.status} server error. Sleeping {delay:.1f}s for {url}\")\n",
        "                    await asyncio.sleep(delay)\n",
        "                    continue\n",
        "                if resp.status != 200:\n",
        "                    text = await resp.text()\n",
        "                    print(f\"HTTP {resp.status} for {url}. Body (trim): {text[:200]}\")\n",
        "                    return None\n",
        "                return await resp.json()\n",
        "        except asyncio.TimeoutError:\n",
        "            delay = _sleep_backoff(attempt, base=2.0)\n",
        "            print(f\"Timeout. Sleeping {delay:.1f}s for {url}\")\n",
        "            await asyncio.sleep(delay)\n",
        "        except aiohttp.ClientError as e:\n",
        "            delay = _sleep_backoff(attempt, base=2.0)\n",
        "            print(f\"ClientError {e}. Sleeping {delay:.1f}s for {url}\")\n",
        "            await asyncio.sleep(delay)\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Count posts per character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def get_post_count(session: aiohttp.ClientSession, query: ScrapeQuery) -> int:\n",
        "    data = await danbooru_get_json(\n",
        "        session,\n",
        "        \"/counts/posts.json\",\n",
        "        params={\"tags\": query.tag_string()},\n",
        "    )\n",
        "    if not data:\n",
        "        return 0\n",
        "    # response shape: { \"counts\": { \"posts\": N, ... } }\n",
        "    return int(data.get(\"counts\", {}).get(\"posts\", 0))\n",
        "\n",
        "async def count_all(characters: Dict[str, str]) -> Dict[str, int]:\n",
        "    timeout = aiohttp.ClientTimeout(total=None)\n",
        "    connector = aiohttp.TCPConnector(limit=MAX_CONN)\n",
        "    async with aiohttp.ClientSession(auth=AUTH, timeout=timeout, connector=connector) as session:\n",
        "        out = {}\n",
        "        for cls, tag in characters.items():\n",
        "            q = ScrapeQuery(tag=tag)\n",
        "            n = await get_post_count(session, q)\n",
        "            out[cls] = n\n",
        "            print(f\"{cls:>20}  tag={tag:<30}  posts={n}\")\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f1afa887",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 Ace  tag=portgas_d._ace                  posts=579\n",
            "              Akainu  tag=sakazuki_(one_piece)            posts=51\n",
            "               Brook  tag=brook_(one_piece)               posts=191\n",
            "             Chopper  tag=tony_tony_chopper               posts=150\n",
            "           Crocodile  tag=crocodile_(one_piece)           posts=430\n",
            "              Franky  tag=franky_(one_piece)              posts=110\n",
            "              Jinbei  tag=jinbe_(one_piece)               posts=166\n",
            "            Kurohige  tag=marshall_d._teach               posts=42\n",
            "                 Law  tag=trafalgar_law                   posts=951\n",
            "               Luffy  tag=monkey_d._luffy                 posts=1263\n",
            "              Mihawk  tag=dracule_mihawk                  posts=171\n",
            "                Nami  tag=nami_(one_piece)                posts=3470\n",
            "            Rayleigh  tag=silvers_rayleigh                posts=86\n",
            "               Robin  tag=nico_robin                      posts=2290\n",
            "               Sanji  tag=sanji_(one_piece)               posts=1043\n",
            "              Shanks  tag=shanks_(one_piece)              posts=243\n",
            "               Usopp  tag=usopp                           posts=113\n",
            "                Zoro  tag=roronoa_zoro                    posts=1253\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Ace': 579,\n",
              " 'Akainu': 51,\n",
              " 'Brook': 191,\n",
              " 'Chopper': 150,\n",
              " 'Crocodile': 430,\n",
              " 'Franky': 110,\n",
              " 'Jinbei': 166,\n",
              " 'Kurohige': 42,\n",
              " 'Law': 951,\n",
              " 'Luffy': 1263,\n",
              " 'Mihawk': 171,\n",
              " 'Nami': 3470,\n",
              " 'Rayleigh': 86,\n",
              " 'Robin': 2290,\n",
              " 'Sanji': 1043,\n",
              " 'Shanks': 243,\n",
              " 'Usopp': 113,\n",
              " 'Zoro': 1253}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nest_asyncio.apply()\n",
        "counts = asyncio.run(count_all(characters_dict))\n",
        "counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Enumerate post metadata and collect file URLs\n",
        "We call `GET /posts.json` with `limit=200` and `page=1..N`.\n",
        "\n",
        "Then we keep only posts whose `file_ext` is in `{'jpg','png'}` and that contain a usable URL (`file_url`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pages_needed(total_posts: int, per_page: int = POSTS_LIMIT_PER_PAGE) -> int:\n",
        "    return int(math.ceil(total_posts / per_page)) if total_posts > 0 else 0\n",
        "\n",
        "async def fetch_posts_page(\n",
        "    session: aiohttp.ClientSession,\n",
        "    query: ScrapeQuery,\n",
        "    page: int,\n",
        "    limit: int = POSTS_LIMIT_PER_PAGE,\n",
        ") -> List[dict]:\n",
        "    data = await danbooru_get_json(\n",
        "        session,\n",
        "        \"/posts.json\",\n",
        "        params={\"tags\": query.tag_string(), \"page\": page, \"limit\": limit},\n",
        "    )\n",
        "    return data if isinstance(data, list) else []\n",
        "\n",
        "def pick_download_url(post: dict) -> Optional[str]:\n",
        "    # Prefer original file_url if present.\n",
        "    # Danbooru may also provide large_file_url / preview_file_url, but we want originals.\n",
        "    url = post.get(\"file_url\")\n",
        "    if not url:\n",
        "        return None\n",
        "    # file_url sometimes may be protocol-relative; normalize\n",
        "    if url.startswith(\"//\"):\n",
        "        url = \"https:\" + url\n",
        "    return url\n",
        "\n",
        "async def collect_urls_for_tag(\n",
        "    session: aiohttp.ClientSession,\n",
        "    tag: str,\n",
        "    total_posts: int,\n",
        "    *,\n",
        "    seen_md5: set[str],\n",
        "    min_width: int = MIN_WIDTH,\n",
        "    min_height: int = MIN_HEIGHT,\n",
        ") -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"\n",
        "    Collect unique (by md5) download URLs for a given character tag.\n",
        "\n",
        "    Deduplication:\n",
        "      - Danbooru exposes an 'md5' field per post. If two posts point to the same file bytes,\n",
        "        they share the same md5 even if their post IDs differ.\n",
        "      - We keep a shared `seen_md5` set (optionally global across classes) and skip any post\n",
        "        whose md5 we already collected.\n",
        "    \"\"\"\n",
        "    q = ScrapeQuery(tag=tag)\n",
        "    n_pages = pages_needed(total_posts)\n",
        "    urls: List[str] = []\n",
        "    post_ids: List[int] = []\n",
        "\n",
        "    def ok_size(p: dict) -> bool:\n",
        "        w = int(p.get(\"image_width\") or 0)\n",
        "        h = int(p.get(\"image_height\") or 0)\n",
        "        if min_width and w < min_width:\n",
        "            return False\n",
        "        if min_height and h < min_height:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    for page in range(1, n_pages + 1):\n",
        "        posts = await fetch_posts_page(session, q, page=page)\n",
        "        if not posts:\n",
        "            continue\n",
        "\n",
        "        for p in posts:\n",
        "            ext = (p.get(\"file_ext\") or \"\\\\\").lower()\n",
        "            if ext not in ALLOWED_EXTS:\n",
        "                continue\n",
        "\n",
        "            if not ok_size(p):\n",
        "                continue\n",
        "\n",
        "            md5 = (p.get(\"md5\") or \"\\\\\").lower()\n",
        "            if md5:\n",
        "                if md5 in seen_md5:\n",
        "                    continue\n",
        "                seen_md5.add(md5)\n",
        "\n",
        "            url = pick_download_url(p)\n",
        "            if not url:\n",
        "                continue\n",
        "\n",
        "            urls.append(url)\n",
        "            if \"id\" in p:\n",
        "                post_ids.append(int(p[\"id\"]))\n",
        "\n",
        "        if page % 5 == 0 or page == n_pages:\n",
        "            print(f\"  page {page}/{n_pages}  collected={len(urls)}  unique_md5={len(seen_md5)}\")\n",
        "\n",
        "    return urls, post_ids\n",
        "\n",
        "async def collect_all_urls(characters: Dict[str, str], counts: Dict[str, int]) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Collect URLs for all classes.\n",
        "\n",
        "    Dedup behavior:\n",
        "      - A single `seen_md5` set is shared across ALL classes, so the same underlying file bytes\n",
        "        won't be downloaded twice even if it appears under multiple posts/tags.\n",
        "      - With chartags:1 enabled, cross-class duplicates should be rare; when they happen, they're\n",
        "        often mistags or re-uploads, so global dedup is usually desirable.\n",
        "    \"\"\"\n",
        "    timeout = aiohttp.ClientTimeout(total=None)\n",
        "    connector = aiohttp.TCPConnector(limit=MAX_CONN)\n",
        "    async with aiohttp.ClientSession(auth=AUTH, timeout=timeout, connector=connector) as session:\n",
        "        out = {}\n",
        "        seen_md5: set[str] = set()\n",
        "\n",
        "        for cls, tag in characters.items():\n",
        "            seen_md5 = set()\n",
        "            total = counts.get(cls, 0)\n",
        "            print(f\"Collecting URLs for {cls} (tag={tag}, total_posts={total})\")\n",
        "            urls, _ = await collect_urls_for_tag(session, tag, total_posts=total, seen_md5=seen_md5)\n",
        "            out[cls] = urls\n",
        "            print(f\"  -> {len(urls)} image URLs (global unique md5 so far: {len(seen_md5)})\")\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ee76b1f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting URLs for Ace (tag=portgas_d._ace, total_posts=579)\n",
            "  page 3/3  collected=524  unique_md5=525\n",
            "  -> 524 image URLs (global unique md5 so far: 525)\n",
            "Collecting URLs for Akainu (tag=sakazuki_(one_piece), total_posts=51)\n",
            "  page 1/1  collected=49  unique_md5=49\n",
            "  -> 49 image URLs (global unique md5 so far: 49)\n",
            "Collecting URLs for Brook (tag=brook_(one_piece), total_posts=191)\n",
            "  page 1/1  collected=180  unique_md5=180\n",
            "  -> 180 image URLs (global unique md5 so far: 180)\n",
            "Collecting URLs for Chopper (tag=tony_tony_chopper, total_posts=150)\n",
            "  page 1/1  collected=132  unique_md5=133\n",
            "  -> 132 image URLs (global unique md5 so far: 133)\n",
            "Collecting URLs for Crocodile (tag=crocodile_(one_piece), total_posts=430)\n",
            "  page 3/3  collected=393  unique_md5=394\n",
            "  -> 393 image URLs (global unique md5 so far: 394)\n",
            "Collecting URLs for Franky (tag=franky_(one_piece), total_posts=110)\n",
            "  page 1/1  collected=97  unique_md5=98\n",
            "  -> 97 image URLs (global unique md5 so far: 98)\n",
            "Collecting URLs for Jinbei (tag=jinbe_(one_piece), total_posts=166)\n",
            "  page 1/1  collected=157  unique_md5=158\n",
            "  -> 157 image URLs (global unique md5 so far: 158)\n",
            "Collecting URLs for Kurohige (tag=marshall_d._teach, total_posts=42)\n",
            "  page 1/1  collected=39  unique_md5=40\n",
            "  -> 39 image URLs (global unique md5 so far: 40)\n",
            "Collecting URLs for Law (tag=trafalgar_law, total_posts=951)\n",
            "  page 5/5  collected=909  unique_md5=910\n",
            "  -> 909 image URLs (global unique md5 so far: 910)\n",
            "Collecting URLs for Luffy (tag=monkey_d._luffy, total_posts=1263)\n",
            "  page 5/7  collected=924  unique_md5=925\n",
            "  page 7/7  collected=1156  unique_md5=1157\n",
            "  -> 1156 image URLs (global unique md5 so far: 1157)\n",
            "Collecting URLs for Mihawk (tag=dracule_mihawk, total_posts=171)\n",
            "  page 1/1  collected=160  unique_md5=160\n",
            "  -> 160 image URLs (global unique md5 so far: 160)\n",
            "Collecting URLs for Nami (tag=nami_(one_piece), total_posts=3470)\n",
            "  page 5/18  collected=970  unique_md5=971\n",
            "  page 10/18  collected=1934  unique_md5=1935\n",
            "  page 15/18  collected=2912  unique_md5=2913\n",
            "  page 18/18  collected=3272  unique_md5=3273\n",
            "  -> 3272 image URLs (global unique md5 so far: 3273)\n",
            "Collecting URLs for Rayleigh (tag=silvers_rayleigh, total_posts=86)\n",
            "  page 1/1  collected=79  unique_md5=79\n",
            "  -> 79 image URLs (global unique md5 so far: 79)\n",
            "Collecting URLs for Robin (tag=nico_robin, total_posts=2290)\n",
            "  page 5/12  collected=965  unique_md5=966\n",
            "  page 10/12  collected=1924  unique_md5=1925\n",
            "  page 12/12  collected=2130  unique_md5=2131\n",
            "  -> 2130 image URLs (global unique md5 so far: 2131)\n",
            "Collecting URLs for Sanji (tag=sanji_(one_piece), total_posts=1043)\n",
            "  page 5/6  collected=956  unique_md5=957\n",
            "  page 6/6  collected=980  unique_md5=981\n",
            "  -> 980 image URLs (global unique md5 so far: 981)\n",
            "Collecting URLs for Shanks (tag=shanks_(one_piece), total_posts=243)\n",
            "  page 2/2  collected=228  unique_md5=228\n",
            "  -> 228 image URLs (global unique md5 so far: 228)\n",
            "Collecting URLs for Usopp (tag=usopp, total_posts=113)\n",
            "  page 1/1  collected=99  unique_md5=99\n",
            "  -> 99 image URLs (global unique md5 so far: 99)\n",
            "Collecting URLs for Zoro (tag=roronoa_zoro, total_posts=1253)\n",
            "  page 5/7  collected=956  unique_md5=956\n",
            "  page 7/7  collected=1188  unique_md5=1189\n",
            "  -> 1188 image URLs (global unique md5 so far: 1189)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Ace': 524,\n",
              " 'Akainu': 49,\n",
              " 'Brook': 180,\n",
              " 'Chopper': 132,\n",
              " 'Crocodile': 393,\n",
              " 'Franky': 97,\n",
              " 'Jinbei': 157,\n",
              " 'Kurohige': 39,\n",
              " 'Law': 909,\n",
              " 'Luffy': 1156,\n",
              " 'Mihawk': 160,\n",
              " 'Nami': 3272,\n",
              " 'Rayleigh': 79,\n",
              " 'Robin': 2130,\n",
              " 'Sanji': 980,\n",
              " 'Shanks': 228,\n",
              " 'Usopp': 99,\n",
              " 'Zoro': 1188}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_urls = asyncio.run(collect_all_urls(characters_dict, counts))\n",
        "{k: len(v) for k, v in all_urls.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Download images (jpg + png)\n",
        "Downloads are streamed to disk. We keep concurrency bounded with a semaphore.\n",
        "If `SKIP_IF_EXISTS=True`, re-running the notebook won't re-download existing files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "dl_semaphore = asyncio.Semaphore(MAX_DL_CONCURRENCY)\n",
        "\n",
        "def safe_filename_from_url(url: str) -> str:\n",
        "    # Use original basename, but guard against weird query strings\n",
        "    base = Path(url.split(\"?\")[0]).name\n",
        "    base = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", base)\n",
        "    if not base:\n",
        "        # fallback to hash\n",
        "        base = hashlib.sha1(url.encode(\"utf-8\")).hexdigest()\n",
        "    return base\n",
        "\n",
        "async def download_one(session: aiohttp.ClientSession, url: str, out_dir: Path) -> bool:\n",
        "    name = safe_filename_from_url(url)\n",
        "    out_path = out_dir / name\n",
        "    if SKIP_IF_EXISTS and out_path.exists():\n",
        "        return True\n",
        "\n",
        "    tmp_path = out_path.with_suffix(out_path.suffix + \".part\")\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        await rate_limiter.wait()\n",
        "        try:\n",
        "            async with dl_semaphore:\n",
        "                async with session.get(url, timeout=REQUEST_TIMEOUT_S) as resp:\n",
        "                    if resp.status == 429:\n",
        "                        retry_after = resp.headers.get(\"Retry-After\")\n",
        "                        delay = float(retry_after) if retry_after else _sleep_backoff(attempt, base=2.0)\n",
        "                        print(f\"429 throttled. Sleeping {delay:.1f}s for download {url}\")\n",
        "                        await asyncio.sleep(delay)\n",
        "                        continue\n",
        "                    if resp.status != 200:\n",
        "                        delay = _sleep_backoff(attempt, base=1.5)\n",
        "                        print(f\"HTTP {resp.status} downloading {url}. Retry in {delay:.1f}s\")\n",
        "                        await asyncio.sleep(delay)\n",
        "                        continue\n",
        "\n",
        "                    with open(tmp_path, \"wb\") as f:\n",
        "                        async for chunk in resp.content.iter_chunked(1 << 15):\n",
        "                            f.write(chunk)\n",
        "\n",
        "            tmp_path.replace(out_path)\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            delay = _sleep_backoff(attempt, base=1.5)\n",
        "            print(f\"Error {e} downloading {url}. Retry in {delay:.1f}s\")\n",
        "            await asyncio.sleep(delay)\n",
        "\n",
        "    # cleanup temp\n",
        "    if tmp_path.exists():\n",
        "        tmp_path.unlink(missing_ok=True)\n",
        "    return False\n",
        "\n",
        "async def download_for_class(cls: str, urls: List[str]) -> Tuple[int, int]:\n",
        "    out_dir = OUT_TRAIN_DIR / cls\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timeout = aiohttp.ClientTimeout(total=None)\n",
        "    connector = aiohttp.TCPConnector(limit=MAX_CONN)\n",
        "    async with aiohttp.ClientSession(auth=AUTH, timeout=timeout, connector=connector) as session:\n",
        "        ok = 0\n",
        "        bad = 0\n",
        "        # Create tasks in moderate chunks (avoid creating 10k tasks at once)\n",
        "        chunk_size = 200\n",
        "        for i in range(0, len(urls), chunk_size):\n",
        "            chunk = urls[i:i+chunk_size]\n",
        "            results = await asyncio.gather(*(download_one(session, u, out_dir) for u in chunk))\n",
        "            ok += sum(bool(r) for r in results)\n",
        "            bad += sum(not bool(r) for r in results)\n",
        "            print(f\"{cls}: {min(i+chunk_size, len(urls))}/{len(urls)}  ok={ok} bad={bad}\")\n",
        "        return ok, bad\n",
        "\n",
        "async def download_all(all_urls: Dict[str, List[str]]) -> Dict[str, Tuple[int,int]]:\n",
        "    results = {}\n",
        "    for cls, urls in all_urls.items():\n",
        "        print(f\"Downloading {cls}: {len(urls)} files...\")\n",
        "        ok, bad = await download_for_class(cls, urls)\n",
        "        results[cls] = (ok, bad)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "81e360d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Ace: 524 files...\n",
            "Ace: 200/524  ok=200 bad=0\n",
            "Ace: 400/524  ok=400 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/9d/0e/9d0e4222462a19fbffe8126912cbc766.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/13/25/132563f2a20ce113f0fa657e37b55df4.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/cd/71/cd7176f62f50fb9fb6537503d2372c5c.png. Retry in 1.5s\n",
            "Ace: 524/524  ok=524 bad=0\n",
            "Downloading Akainu: 49 files...\n",
            "Akainu: 49/49  ok=49 bad=0\n",
            "Downloading Brook: 180 files...\n",
            "Error  downloading https://cdn.donmai.us/original/0f/94/0f94b4d165c5b233def1b35e179b80dc.jpg. Retry in 1.5s\n",
            "Brook: 180/180  ok=180 bad=0\n",
            "Downloading Chopper: 132 files...\n",
            "Chopper: 132/132  ok=132 bad=0\n",
            "Downloading Crocodile: 393 files...\n",
            "Crocodile: 200/393  ok=200 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/d7/22/d72261ed284f478d0655ef66afe3ae27.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/9f/30/9f300f62dd5afb043f477e81875504ff.jpg. Retry in 1.5s\n",
            "Crocodile: 393/393  ok=393 bad=0\n",
            "Downloading Franky: 97 files...\n",
            "Franky: 97/97  ok=97 bad=0\n",
            "Downloading Jinbei: 157 files...\n",
            "Jinbei: 157/157  ok=157 bad=0\n",
            "Downloading Kurohige: 39 files...\n",
            "Kurohige: 39/39  ok=39 bad=0\n",
            "Downloading Law: 909 files...\n",
            "Law: 200/909  ok=200 bad=0\n",
            "Law: 400/909  ok=400 bad=0\n",
            "Law: 600/909  ok=600 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/c7/fa/c7fa106d5570023605875e2429c21957.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/a3/aa/a3aa30c16f6d0a2a4f6525b1453f0616.jpg. Retry in 1.5s\n",
            "Law: 800/909  ok=800 bad=0\n",
            "Law: 909/909  ok=909 bad=0\n",
            "Downloading Luffy: 1156 files...\n",
            "Luffy: 200/1156  ok=200 bad=0\n",
            "Luffy: 400/1156  ok=400 bad=0\n",
            "Luffy: 600/1156  ok=600 bad=0\n",
            "Luffy: 800/1156  ok=800 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/90/ea/90ea40f24320db2f26f795261322af21.jpg. Retry in 1.5s\n",
            "Luffy: 1000/1156  ok=1000 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/ce/7c/ce7cce790d90501a276b42e10ff66c26.jpg. Retry in 1.5s\n",
            "Luffy: 1156/1156  ok=1156 bad=0\n",
            "Downloading Mihawk: 160 files...\n",
            "Mihawk: 160/160  ok=160 bad=0\n",
            "Downloading Nami: 3272 files...\n",
            "Error  downloading https://cdn.donmai.us/original/69/64/69643c977ffa518fa811d07d4b5ba10b.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/50/cd/50cd10f0c19d5c873d876b5f932774d3.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/aa/27/aa27dd5ec4674e3b8d758a94d30ab675.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/b5/c8/b5c82705734a3a3b34df34847a04e3db.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/51/9a/519aa8d90be9b8189e7d2a4eff03288e.jpg. Retry in 1.5s\n",
            "Nami: 200/3272  ok=200 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/12/58/1258b07a01eae5997590f99c3cf96c53.png. Retry in 1.5s\n",
            "Nami: 400/3272  ok=400 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/19/9f/199f47574fa763ca6dfa04d406c9a015.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/cf/34/cf343da5ef5c8e2fafac02730b231b5a.png. Retry in 1.5s\n",
            "Nami: 600/3272  ok=600 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/6c/b7/6cb75164ced58c2c4b443ef20b339440.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/d9/d7/d9d7a3161cb4ef2c6fb498bf5c3ae88b.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/57/e9/57e9b72339ff3e71bb28ccb4ce85f56d.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/e4/e9/e4e9acad86bf10b9191db402348da74a.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/7f/a6/7fa6a1aa3071359dd58bdae3efa70ba2.jpg. Retry in 1.5s\n",
            "Nami: 800/3272  ok=800 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/ae/c7/aec75b765c6bd03eb9b39e7baa0bc80c.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/d5/08/d50857d18e4ebb2230edfb425a113c4e.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/12/8f/128f406e72233e44e174a60f62e83a2b.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/47/2e/472e4c8fdd04866e28e3355311ea5501.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/4a/b6/4ab62f77b5905d686378c64cb23be685.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/05/97/0597d07b4d2248190c651d2c13acb465.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/03/bf/03bf7fbadeee8f26af73b91ba9260d33.png. Retry in 1.5s\n",
            "Nami: 1000/3272  ok=1000 bad=0\n",
            "Nami: 1200/3272  ok=1200 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/e9/42/e942aa0c31ebaed48280e67d83d1a3d7.png. Retry in 1.5s\n",
            "Nami: 1400/3272  ok=1400 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/a6/2a/a62a1edab2210cd7cde2a035dfd2c5d7.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/f3/97/f3973718eaf78dd8bdb720edb27c1df7.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/17/d9/17d91b67a023691155cfd288ccf2c3f2.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/8e/8f/8e8fe5b8640bb2093e650b10cd3c9e14.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/ce/35/ce35e2c0641ed8232a8d4f1a433d8f0a.jpg. Retry in 1.5s\n",
            "Nami: 1600/3272  ok=1600 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/75/a2/75a29fb3734356d144ba00742f0328de.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/fc/16/fc162c2f0fa5e9db34b200c369698c91.jpg. Retry in 1.5s\n",
            "Nami: 1800/3272  ok=1800 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/7c/8f/7c8fabafeb4d93a1403b9e6d41868b1d.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/af/9e/af9e4adbc4f5b1e6d79c7ffd93f01f05.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/78/99/78997410692895f676ae288d31f5ecb3.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/8d/da/8ddafce7e2f7ff5a03ad5a5d0f625deb.jpg. Retry in 1.5s\n",
            "Nami: 2000/3272  ok=2000 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/ce/f5/cef598c4d12639e84a6ec2a019e6d125.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/5d/72/5d7221bd0307a3df5d5d1e6888c46e4f.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/4d/63/4d63c6aa48c7d88a5dec7a81826c98c0.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/16/28/1628077bfcf79c1e9bc9fc2ab79b0070.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/10/df/10df6a26f9f7f50085e664c44d2394dd.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/db/78/db78bc02602c46603145c368cfe8c088.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/26/70/2670c5add0296842dc50b496beaa04a6.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/ce/d5/ced5cddae8e0fefa54d9ce99259e6eac.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/85/aa/85aad79f48b33e511b159d0de7b997ad.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/26/70/2670c5add0296842dc50b496beaa04a6.png. Retry in 3.0s\n",
            "Nami: 2200/3272  ok=2200 bad=0\n",
            "Nami: 2400/3272  ok=2400 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/57/b6/57b61fe021229c95727d851fb9ac9265.png. Retry in 1.5s\n",
            "Nami: 2600/3272  ok=2600 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/46/24/4624e0d41cc018ddf64cc4587f39881b.jpg. Retry in 1.5s\n",
            "Nami: 2800/3272  ok=2800 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/ef/35/ef354cb7e2b4c253a7cb3c8275a965cb.jpg. Retry in 1.5s\n",
            "Nami: 3000/3272  ok=3000 bad=0\n",
            "Nami: 3200/3272  ok=3200 bad=0\n",
            "Nami: 3272/3272  ok=3272 bad=0\n",
            "Downloading Rayleigh: 79 files...\n",
            "Rayleigh: 79/79  ok=79 bad=0\n",
            "Downloading Robin: 2130 files...\n",
            "Error  downloading https://cdn.donmai.us/original/ae/34/ae34c7dcead145676300d3ff5d12d5f0.jpg. Retry in 1.5s\n",
            "Robin: 200/2130  ok=200 bad=0\n",
            "Robin: 400/2130  ok=400 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/ec/59/ec59e757d4fbfb0df9eecd3a0c4598ca.jpg. Retry in 1.5s\n",
            "Robin: 600/2130  ok=600 bad=0\n",
            "Robin: 800/2130  ok=800 bad=0\n",
            "Robin: 1000/2130  ok=1000 bad=0\n",
            "Robin: 1200/2130  ok=1200 bad=0\n",
            "Robin: 1400/2130  ok=1400 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/49/83/4983474e87051f954f1f4d92b918e126.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/7f/29/7f296b1e620862b923e94f27b16cdc03.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/10/77/1077f1d64fe63a72dd60d7e4bbeed122.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/c7/5e/c75eb7a9c780be7de2b0e462f93e359b.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/b0/df/b0dff833397e07147cb389e18dee9700.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/3a/92/3a92a0d3ba5b3dcc19211c8698d2b7cb.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/bc/36/bc36a53f8e3074ff47d752997381bc7d.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/30/92/3092b68c651c5569e34344b7b782c7a3.png. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/5b/27/5b2708ad7189cf90ccb39e3aa34ff6db.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/c7/5e/c75eb7a9c780be7de2b0e462f93e359b.jpg. Retry in 3.0s\n",
            "Robin: 1600/2130  ok=1600 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/97/82/97823a4c15e86c896ebcd4d0e43c14b2.png. Retry in 1.5s\n",
            "Robin: 1800/2130  ok=1800 bad=0\n",
            "Robin: 2000/2130  ok=2000 bad=0\n",
            "Robin: 2130/2130  ok=2130 bad=0\n",
            "Downloading Sanji: 980 files...\n",
            "Sanji: 200/980  ok=200 bad=0\n",
            "Sanji: 400/980  ok=400 bad=0\n",
            "Sanji: 600/980  ok=600 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/91/0b/910be2f7e7153e85254bcb1dedbe2cda.jpg. Retry in 1.5s\n",
            "Sanji: 800/980  ok=800 bad=0\n",
            "Sanji: 980/980  ok=980 bad=0\n",
            "Downloading Shanks: 228 files...\n",
            "Error  downloading https://cdn.donmai.us/original/37/3b/373b15b211f18b42ac6e5f3073fbc5d8.png. Retry in 1.5s\n",
            "Shanks: 200/228  ok=200 bad=0\n",
            "Shanks: 228/228  ok=228 bad=0\n",
            "Downloading Usopp: 99 files...\n",
            "Error  downloading https://cdn.donmai.us/original/49/86/4986435c6973c37839a4a4ee68457d15.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/70/6e/706e1fc733ee7879f0c3af8d1fef75da.jpg. Retry in 1.5s\n",
            "Usopp: 99/99  ok=99 bad=0\n",
            "Downloading Zoro: 1188 files...\n",
            "Zoro: 200/1188  ok=200 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/31/4c/314c489cc0b1d01a7727ecbafe8dad3e.png. Retry in 1.5s\n",
            "Zoro: 400/1188  ok=400 bad=0\n",
            "Zoro: 600/1188  ok=600 bad=0\n",
            "Zoro: 800/1188  ok=800 bad=0\n",
            "Error  downloading https://cdn.donmai.us/original/93/86/9386827c64a53014d2a9f78cf9424de4.jpg. Retry in 1.5s\n",
            "Error  downloading https://cdn.donmai.us/original/90/b3/90b397a47008e4ad10661067997b186f.jpg. Retry in 1.5s\n",
            "Zoro: 1000/1188  ok=1000 bad=0\n",
            "Zoro: 1188/1188  ok=1188 bad=0\n",
            "Done in 2365.9s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'Ace': (524, 0),\n",
              " 'Akainu': (49, 0),\n",
              " 'Brook': (180, 0),\n",
              " 'Chopper': (132, 0),\n",
              " 'Crocodile': (393, 0),\n",
              " 'Franky': (97, 0),\n",
              " 'Jinbei': (157, 0),\n",
              " 'Kurohige': (39, 0),\n",
              " 'Law': (909, 0),\n",
              " 'Luffy': (1156, 0),\n",
              " 'Mihawk': (160, 0),\n",
              " 'Nami': (3272, 0),\n",
              " 'Rayleigh': (79, 0),\n",
              " 'Robin': (2130, 0),\n",
              " 'Sanji': (980, 0),\n",
              " 'Shanks': (228, 0),\n",
              " 'Usopp': (99, 0),\n",
              " 'Zoro': (1188, 0)}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start = time.perf_counter()\n",
        "dl_stats = asyncio.run(download_all(all_urls))\n",
        "elapsed = time.perf_counter() - start\n",
        "\n",
        "print(f\"Done in {elapsed:.1f}s\")\n",
        "dl_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Validate and delete corrupted images (optional)\n",
        "PIL's `verify()` can catch truncated/invalid files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_corrupted(root: Path) -> List[Path]:\n",
        "    bad = []\n",
        "    for p in root.rglob(\"*\"):\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        try:\n",
        "            with Image.open(p) as img:\n",
        "                img.verify()\n",
        "        except Exception:\n",
        "            bad.append(p)\n",
        "    return bad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "fd650148",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrupted: 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bad_files = find_corrupted(OUT_TRAIN_DIR)\n",
        "print(\"Corrupted:\", len(bad_files))\n",
        "bad_files[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted 0 files\n"
          ]
        }
      ],
      "source": [
        "# Delete corrupted files\n",
        "for p in bad_files:\n",
        "    p.unlink(missing_ok=True)\n",
        "print(\"Deleted\", len(bad_files), \"files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Final counts per class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Ace': 524,\n",
              " 'Akainu': 49,\n",
              " 'Brook': 180,\n",
              " 'Chopper': 132,\n",
              " 'Crocodile': 393,\n",
              " 'Franky': 97,\n",
              " 'Jinbei': 157,\n",
              " 'Kurohige': 39,\n",
              " 'Law': 909,\n",
              " 'Luffy': 1156,\n",
              " 'Mihawk': 160,\n",
              " 'Nami': 3272,\n",
              " 'Rayleigh': 79,\n",
              " 'Robin': 2130,\n",
              " 'Sanji': 980,\n",
              " 'Shanks': 228,\n",
              " 'Usopp': 99,\n",
              " 'Zoro': 1188}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counts_after = {}\n",
        "for cls in characters_list:\n",
        "    counts_after[cls] = sum(1 for _ in (OUT_TRAIN_DIR/cls).glob(\"*\") if _.is_file())\n",
        "\n",
        "counts_after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e7d934",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "wsl python 3.11.5",
      "language": "python",
      "name": "wsl_main"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
